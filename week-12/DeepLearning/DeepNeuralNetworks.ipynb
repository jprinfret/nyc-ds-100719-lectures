{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "How does sklearn utilize numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn uses numpy arrays since they are much faster than Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy is to sklearn as tensor_flow is to kerras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a Deep Neural Network with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building steps:\n",
    "\n",
    "1. Specify Architecture\n",
    "\n",
    "2. Compile\n",
    "\n",
    "3. Fit \n",
    "\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "#create a hidden later and the input layer\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "\n",
    "#add one hidden layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "#add the final layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a model \n",
    "\n",
    "- Specify the optimizer\n",
    "    - Many options and mathematically complex\n",
    "    - “Adam” is usually a good choice \n",
    "- Loss function\n",
    "    - “mean_squared_error” common for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "- Applying backpropagation and gradient descent with your data to update the weights\n",
    "- Scaling data before fi!ing can ease optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c653c5808ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Some output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Number of observations per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                       validation_data=(X_test, y_test)) # Data for evaluation\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "model.fit(features, # Features\n",
    "#                       target, # Target\n",
    "                      epochs=15, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied:Create a Regression NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-mhtn-ds-042219-lectures/master/Module_4/kc_feat_engineering_project_revamp/kc_housing_data_for_feat_engineering_lab.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>...</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>yr_old</th>\n",
       "      <th>year_sold</th>\n",
       "      <th>since_sold</th>\n",
       "      <th>price_log</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7129300520</th>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td>62</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>12.309982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414100192</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td>66</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.195614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631500400</th>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td>84</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>12.100712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487200875</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td>52</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>13.311329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954400510</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td>30</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>13.142166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "id                                                                             \n",
       "7129300520  2014-10-13  221900.0         3       1.00         1180      5650   \n",
       "6414100192  2014-12-09  538000.0         3       2.25         2570      7242   \n",
       "5631500400  2015-02-25  180000.0         2       1.00          770     10000   \n",
       "2487200875  2014-12-09  604000.0         4       3.00         1960      5000   \n",
       "1954400510  2015-02-18  510000.0         3       2.00         1680      8080   \n",
       "\n",
       "            floors  waterfront  view  condition  ...  yr_renovated  zipcode  \\\n",
       "id                                               ...                          \n",
       "7129300520     1.0           0     0          3  ...             0    98178   \n",
       "6414100192     2.0           0     0          3  ...          1991    98125   \n",
       "5631500400     1.0           0     0          3  ...             0    98028   \n",
       "2487200875     1.0           0     0          5  ...             0    98136   \n",
       "1954400510     1.0           0     0          3  ...             0    98074   \n",
       "\n",
       "                lat     long  sqft_living15  sqft_lot15  yr_old  year_sold  \\\n",
       "id                                                                           \n",
       "7129300520  47.5112 -122.257           1340        5650      62       2014   \n",
       "6414100192  47.7210 -122.319           1690        7639      66       2014   \n",
       "5631500400  47.7379 -122.233           2720        8062      84       2015   \n",
       "2487200875  47.5208 -122.393           1360        5000      52       2014   \n",
       "1954400510  47.6168 -122.045           1800        7503      30       2015   \n",
       "\n",
       "            since_sold  price_log  \n",
       "id                                 \n",
       "7129300520           3  12.309982  \n",
       "6414100192           3  13.195614  \n",
       "5631500400           2  12.100712  \n",
       "2487200875           3  13.311329  \n",
       "1954400510           2  13.142166  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "       'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "       'sqft_living15', 'sqft_lot15', 'yr_old', 'since_sold',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price']\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store the number of columns in the predictors data to `n_cols`. This has been done for you.\n",
    "- Start by creating a `Sequential` model called `model`.\n",
    "- Use the `.add()` method on `model` to add a `Dense` layer.\n",
    "- Add 50 units, specify `activation='relu'`, and the `input_shape` parameter to be the tuple `(n_cols,)` which means it has `n_cols` items in each row of data, and any number of rows of data are acceptable as inputs.\n",
    "- Add another `Dense` layer. This should have 32 units and a 'relu' activation.\n",
    "- Finally, add an output layer, which is a `Dense` layer with a single node. Don't use any activation function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = len(features)\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile the model using `model.compile()`. Your `optimizer` should be `'adam'` and the `loss` should be `'mean_squared_error'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit the `model`. Remember that the first argument is the predictive features (`predictors`), and the data to be predicted (`target`) is the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 21600 samples\n",
      "21600/21600 [==============================] - 3s 154us/sample - loss: 141109576344.4622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6476f49d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "\n",
    "\n",
    "- ‘categorical_crossentropy’ loss function Similar to log loss: Lower is be!er\n",
    "- Add metrics = [‘accuracy’] to compile step for easy-to- understand diagnostics\n",
    "- Output layer has separate node for each possible outcome, and uses ‘so\"max’ activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  \\\n",
       "PassengerId                                                                    \n",
       "1                   0       3  22.0      1      0   7.2500        0     1  0   \n",
       "2                   1       1  38.0      1      0  71.2833        0     0  0   \n",
       "3                   1       3  26.0      0      0   7.9250        0     0  0   \n",
       "4                   1       1  35.0      1      0  53.1000        0     0  0   \n",
       "5                   0       3  35.0      0      0   8.0500        0     1  0   \n",
       "\n",
       "             S  \n",
       "PassengerId     \n",
       "1            1  \n",
       "2            0  \n",
       "3            1  \n",
       "4            1  \n",
       "5            1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-mhtn-ds-042219-lectures/master/Module_4/cleaned_titanic.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = df.drop(columns=['Survived'])\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert `df.Survived` to a categorical variable using the `to_categorical()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Convert the target to categorical: target\n",
    "target = to_categorical(df.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify a `Sequential` model called `model`.\n",
    "- Add a `Dense` layer with 32 nodes. Use `'relu'` as the `activation` and `(n_cols,)` as the `input_shape`.\n",
    "- Add the `Dense` output layer. Because there are two outcomes, it should have 2 units, and because it is a classification model, the `activation` should be `'softmax'`.\n",
    "- Compile the model, using `'sgd'` as the `optimizer`, `'categorical_crossentropy'` as the loss function, and `metrics=['accuracy']` to see the accuracy (what fraction of predictions were correct) at the end of each epoch.\n",
    "- Fit the model using the `X_train` and the `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 711 samples, validate on 178 samples\n",
      "Epoch 1/15\n",
      "711/711 - 1s - loss: 3.1970 - accuracy: 0.5752 - val_loss: 1.3610 - val_accuracy: 0.6966\n",
      "Epoch 2/15\n",
      "711/711 - 0s - loss: 2.3722 - accuracy: 0.5738 - val_loss: 3.1297 - val_accuracy: 0.4045\n",
      "Epoch 3/15\n",
      "711/711 - 0s - loss: 1.7003 - accuracy: 0.6456 - val_loss: 2.5734 - val_accuracy: 0.6124\n",
      "Epoch 4/15\n",
      "711/711 - 0s - loss: 2.2491 - accuracy: 0.5556 - val_loss: 1.7518 - val_accuracy: 0.6067\n",
      "Epoch 5/15\n",
      "711/711 - 0s - loss: 1.2132 - accuracy: 0.6709 - val_loss: 0.8505 - val_accuracy: 0.6517\n",
      "Epoch 6/15\n",
      "711/711 - 0s - loss: 1.1519 - accuracy: 0.6006 - val_loss: 1.3148 - val_accuracy: 0.6067\n",
      "Epoch 7/15\n",
      "711/711 - 0s - loss: 1.0191 - accuracy: 0.6315 - val_loss: 0.9624 - val_accuracy: 0.6292\n",
      "Epoch 8/15\n",
      "711/711 - 0s - loss: 0.8376 - accuracy: 0.6203 - val_loss: 0.6213 - val_accuracy: 0.7247\n",
      "Epoch 9/15\n",
      "711/711 - 0s - loss: 0.7737 - accuracy: 0.6582 - val_loss: 0.7203 - val_accuracy: 0.6180\n",
      "Epoch 10/15\n",
      "711/711 - 0s - loss: 0.7250 - accuracy: 0.6512 - val_loss: 0.8986 - val_accuracy: 0.6124\n",
      "Epoch 11/15\n",
      "711/711 - 0s - loss: 0.7565 - accuracy: 0.6273 - val_loss: 0.6561 - val_accuracy: 0.6854\n",
      "Epoch 12/15\n",
      "711/711 - 0s - loss: 0.6211 - accuracy: 0.6807 - val_loss: 0.9121 - val_accuracy: 0.6124\n",
      "Epoch 13/15\n",
      "711/711 - 0s - loss: 0.6645 - accuracy: 0.6554 - val_loss: 0.8349 - val_accuracy: 0.5899\n",
      "Epoch 14/15\n",
      "711/711 - 0s - loss: 0.7710 - accuracy: 0.6456 - val_loss: 1.3752 - val_accuracy: 0.6124\n",
      "Epoch 15/15\n",
      "711/711 - 0s - loss: 0.7554 - accuracy: 0.6540 - val_loss: 0.5826 - val_accuracy: 0.6966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4b219190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=15, # Number of epochs\n",
    "          verbose=2, # Some output\n",
    "          batch_size=100, # Number of observations per batch\n",
    "          validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving, reloading and using your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('my_model.h5')\n",
    "my_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create your predictions using the model's `.predict()` method on `X_test`.\n",
    "- Use NumPy indexing to find the column corresponding to predicted probabilities of survival being True. This is the second column (index `1`) of `predictions`. Store the result in `predicted_prob_true` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "[0.20251688 0.7271282  0.39317462 0.2627178  0.6982196  0.20192613\n",
      " 0.30670542 0.2629618  0.72851294 0.7076465  0.50049376 0.3160295\n",
      " 0.0708776  0.30345848 0.20940979 0.38016453 0.67413956 0.24545452\n",
      " 0.5313726  0.23103714 0.52003497 0.6560888  0.12632194 0.44180906\n",
      " 0.2325427  0.40319666 0.22207502 0.27312717 0.54006827 0.2697463\n",
      " 0.53068256 0.23830315 0.2325427  0.5420182  0.4278616  0.571381\n",
      " 0.48827565 0.98102367 0.26061714 0.2325427  0.26672268 0.22399016\n",
      " 0.32540584 0.21660332 0.4401494  0.461194   0.5310799  0.22338423\n",
      " 0.23131546 0.88781786 0.58392805 0.27952567 0.24574834 0.28065795\n",
      " 0.5583961  0.54088074 0.15936412 0.132519   0.31295896 0.23805857\n",
      " 0.32758254 0.2904998  0.6395059  0.31628057 0.26426542 0.38148344\n",
      " 0.07909358 0.5633447  0.68238735 0.64458054 0.28861982 0.8044399\n",
      " 0.3987913  0.21624498 0.2303626  0.31026042 0.2745323  0.1519915\n",
      " 0.5089646  0.51989096 0.09751346 0.5451844  0.44016555 0.23830315\n",
      " 0.31373563 0.23558372 0.11276413 0.32540584 0.21801816 0.2325427\n",
      " 0.18371156 0.26917568 0.24024352 0.17050807 0.5673989  0.44567734\n",
      " 0.23103714 0.19697009 0.4446558  0.23558372 0.21627317 0.8428393\n",
      " 0.51099914 0.18087798 0.05525357 0.11075611 0.21240367 0.60492927\n",
      " 0.26157543 0.26053336 0.28445196 0.6241223  0.4630449  0.60573083\n",
      " 0.23289369 0.30366886 0.10118581 0.14467376 0.23142055 0.4332514\n",
      " 0.48939377 0.6901905  0.08616375 0.30146182 0.64711344 0.67285156\n",
      " 0.44180903 0.6187343  0.26161167 0.1699709  0.24845918 0.3632449\n",
      " 0.57255137 0.24541451 0.2188013  0.4671142  0.18255983 0.31828973\n",
      " 0.43700546 0.558433   0.33722484 0.23103714 0.28197193 0.47607264\n",
      " 0.72377044 0.5989121  0.04515809 0.7505746  0.22990628 0.7802865\n",
      " 0.2273788  0.48013154 0.5664563  0.5417127  0.33048928 0.52553207\n",
      " 0.5601982  0.6558461  0.07979506 0.65588635 0.22006235 0.67046577\n",
      " 0.17757471 0.2421935  0.64474803 0.24119605 0.6105124  0.24024352\n",
      " 0.15135711 0.3741885  0.31828973 0.60930103 0.49192882 0.2804266\n",
      " 0.2811346  0.5218116  0.23249246 0.50383466]\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions: predictions\n",
    "predictions = my_model.predict(X_test)\n",
    "\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify your model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Playground](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Number of Hidden Layers**\n",
    "\n",
    "*For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. Very complex tasks, like image classification, will need dozens of layers.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Neurons per layer**\n",
    "\n",
    "*The number of nuerons for the input and output layers are dependent on your data and the task. For hiddne layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.*\n",
    "\n",
    "*In general, you will get more bang for your buck by adding on more layers than adding more neurons.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Activation Functions](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)**\n",
    "    - Linear\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "    - Tanh\n",
    "    - ReLu\n",
    "    - elu\n",
    "    \n",
    "*In most cases you can use the ReLu activation function (or one of its variants) in the hidden layers. For the output layer, the softmax activation function is generally good for multiclass problems and the sigmouid function for binary classificatin problems. For regression tasks, you can simply use no activation function at all*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Selecting an optimizer](https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/)\n",
    "    - Adam\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate**\n",
    "\n",
    "*If you set it too low, training will eventually converge, but it will do so slowly.*\n",
    "*If you set it too high, it might acutally diverge.*\n",
    "*If you set it slightly too high, it will converge at first but miss the local optima.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regularization** \n",
    "    - **L1 and L2**\n",
    "    - **Dropout:**\n",
    "        \n",
    "        *Dropout is most popular techniqure for deep neural networks. It is a fairly simple algorithm where at every training step, every neuron has a probability fo being teporarily \"droppedout,\" meaning it will be completely ignored during this traing step, but it may be active during the next step.*\n",
    "    \n",
    "    - [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "    \n",
    "    *Just interrupt training whne its performance on the validation set starts dropping*\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paper on selecting hyperparameters](https://arxiv.org/pdf/1206.5533v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first network with Keras\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "\n",
    "# Add a dropout layer for input layer\n",
    "network.add(Dropout(0.2, input_shape=(n_cols,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(Dropout(0.1))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "network.add(Dense(units=16, kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "\n",
    "#Final Layer\n",
    "network.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using GridSearchCV to tune Neural Networks](https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Implementation of optimizers](https://keras.io/optimizers/)\n",
    "\n",
    "[Impact of Learning Rate on MOdel Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 711 samples, validate on 178 samples\n",
      "Epoch 1/200\n",
      "711/711 - 3s - loss: 0.6117 - accuracy: 0.7117 - val_loss: 0.5493 - val_accuracy: 0.7697\n",
      "Epoch 2/200\n",
      "711/711 - 3s - loss: 0.5768 - accuracy: 0.7342 - val_loss: 0.4972 - val_accuracy: 0.7921\n",
      "Epoch 3/200\n",
      "711/711 - 3s - loss: 0.5967 - accuracy: 0.7032 - val_loss: 0.5386 - val_accuracy: 0.8146\n",
      "Epoch 4/200\n",
      "711/711 - 3s - loss: 0.5782 - accuracy: 0.7342 - val_loss: 0.5252 - val_accuracy: 0.7753\n",
      "Epoch 5/200\n",
      "711/711 - 3s - loss: 0.5814 - accuracy: 0.7089 - val_loss: 0.5028 - val_accuracy: 0.8090\n",
      "Epoch 6/200\n",
      "711/711 - 3s - loss: 0.5740 - accuracy: 0.7229 - val_loss: 0.5536 - val_accuracy: 0.7753\n",
      "Epoch 7/200\n",
      "711/711 - 3s - loss: 0.5797 - accuracy: 0.7103 - val_loss: 0.5007 - val_accuracy: 0.8034\n",
      "Epoch 8/200\n",
      "711/711 - 3s - loss: 0.5589 - accuracy: 0.7271 - val_loss: 0.5204 - val_accuracy: 0.7809\n",
      "Epoch 9/200\n",
      "711/711 - 3s - loss: 0.5640 - accuracy: 0.7384 - val_loss: 0.5347 - val_accuracy: 0.7528\n",
      "Epoch 10/200\n",
      "711/711 - 3s - loss: 0.5394 - accuracy: 0.7482 - val_loss: 0.4954 - val_accuracy: 0.7921\n",
      "Epoch 11/200\n",
      "711/711 - 3s - loss: 0.5484 - accuracy: 0.7511 - val_loss: 0.4809 - val_accuracy: 0.8034\n",
      "Epoch 12/200\n",
      "711/711 - 3s - loss: 0.5737 - accuracy: 0.7187 - val_loss: 0.5136 - val_accuracy: 0.8034\n",
      "Epoch 13/200\n",
      "711/711 - 3s - loss: 0.5544 - accuracy: 0.7286 - val_loss: 0.4990 - val_accuracy: 0.8034\n",
      "Epoch 14/200\n",
      "711/711 - 3s - loss: 0.5407 - accuracy: 0.7412 - val_loss: 0.5104 - val_accuracy: 0.7753\n",
      "Epoch 15/200\n",
      "711/711 - 3s - loss: 0.5700 - accuracy: 0.7370 - val_loss: 0.5222 - val_accuracy: 0.7584\n",
      "Epoch 16/200\n",
      "711/711 - 3s - loss: 0.5494 - accuracy: 0.7440 - val_loss: 0.4942 - val_accuracy: 0.8090\n",
      "Epoch 17/200\n",
      "711/711 - 3s - loss: 0.5573 - accuracy: 0.7300 - val_loss: 0.4968 - val_accuracy: 0.8202\n",
      "Epoch 18/200\n",
      "711/711 - 3s - loss: 0.5323 - accuracy: 0.7539 - val_loss: 0.4986 - val_accuracy: 0.8034\n",
      "Epoch 19/200\n",
      "711/711 - 3s - loss: 0.5548 - accuracy: 0.7412 - val_loss: 0.5250 - val_accuracy: 0.7472\n",
      "Epoch 20/200\n",
      "711/711 - 3s - loss: 0.5525 - accuracy: 0.7328 - val_loss: 0.5063 - val_accuracy: 0.8090\n",
      "Epoch 21/200\n",
      "711/711 - 3s - loss: 0.5473 - accuracy: 0.7412 - val_loss: 0.5390 - val_accuracy: 0.7640\n",
      "Epoch 22/200\n",
      "711/711 - 3s - loss: 0.5432 - accuracy: 0.7426 - val_loss: 0.5140 - val_accuracy: 0.7753\n",
      "Epoch 23/200\n",
      "711/711 - 3s - loss: 0.5529 - accuracy: 0.7300 - val_loss: 0.4972 - val_accuracy: 0.7978\n",
      "Epoch 24/200\n",
      "711/711 - 3s - loss: 0.5495 - accuracy: 0.7356 - val_loss: 0.4962 - val_accuracy: 0.7865\n",
      "Epoch 25/200\n",
      "711/711 - 3s - loss: 0.5614 - accuracy: 0.7131 - val_loss: 0.5055 - val_accuracy: 0.7978\n",
      "Epoch 26/200\n",
      "711/711 - 3s - loss: 0.5389 - accuracy: 0.7342 - val_loss: 0.4820 - val_accuracy: 0.8090\n",
      "Epoch 27/200\n",
      "711/711 - 3s - loss: 0.5350 - accuracy: 0.7525 - val_loss: 0.5025 - val_accuracy: 0.7809\n",
      "Epoch 28/200\n",
      "711/711 - 3s - loss: 0.5238 - accuracy: 0.7609 - val_loss: 0.4949 - val_accuracy: 0.8090\n",
      "Epoch 29/200\n",
      "711/711 - 3s - loss: 0.5177 - accuracy: 0.7539 - val_loss: 0.4949 - val_accuracy: 0.7978\n",
      "Epoch 30/200\n",
      "711/711 - 3s - loss: 0.5384 - accuracy: 0.7511 - val_loss: 0.5187 - val_accuracy: 0.7640\n",
      "Epoch 31/200\n",
      "711/711 - 3s - loss: 0.5502 - accuracy: 0.7243 - val_loss: 0.5172 - val_accuracy: 0.7697\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=200, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=1, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      callbacks=callbacks) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 9)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "178/178 [==============================] - 0s 33us/sample - loss: 0.6515 - accuracy: 0.6966\n"
     ]
    }
   ],
   "source": [
    "score = network.evaluate(X_test, y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 69.66%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (network.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = network.predict(X_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_loss_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_performance_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "    \n",
    "http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
