{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling Assessment Project\n",
    "\n",
    "Welcome to your Topic Modeling Assessment! For this project you will be working with a dataset of over 400,000 quora questions that have no labeled cateogry, and attempting to find optimal number of cateogries to assign these questions to. The .csv file of these text questions can be found in the NLP folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Import pandas and read in the quora_questions.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JacquesPierre/anaconda3/lib/python3.7/site-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "/Users/JacquesPierre/anaconda3/lib/python3.7/site-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/quora_questions.csv')\n",
    "npr = df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(npr.shape)\n",
    "npr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "#### Task: Create a vectorized document term matrix. \n",
    "\n",
    "- How do you want to clean up your text with regards to stopwords, special characters, and other situations.\n",
    "- Using a Countvectorizer versus a TFIDFvectorizer\n",
    "- You may want to explore the max_df and min_df parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "    # remove html tags from all of the text before processing\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    # we disabled the parser and ner parts of the pipeline in order to speed up parsing\n",
    "    mytokens = nlp(cleantext, disable=['parser', 'ner'])\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=spacy_tokenizer, max_df=0.70, min_df=100, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x34 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6711 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = cv.fit_transform(npr['Question'])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Modelling\n",
    "\n",
    "#### TASK: Using Scikit-Learn create an instance of LDA. \n",
    "\n",
    "- You can manually run and tune your model, then evaluate the resulting clusters. \n",
    "- Or you can use gridsearch to try and identify the best number of topics to use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# Define Search Param\n",
    "param_grid = {\n",
    "    'n_components': [15, 20, 25],\n",
    "    'learning_decay': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=25)\n",
    "\n",
    "# Init Grid Search Class\n",
    "lda_gs_model = GridSearchCV(lda, param_grid=param_grid, cv=3, verbose=2, n_jobs = -1)\n",
    "\n",
    "# Do the Grid Search\n",
    "lda_gs_model.fit(dtm)\n",
    "\n",
    "print(lda_gs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = lda_gs_model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", lda_gs_model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", lda_gs_model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Evaluate the different models you have run and determine which model you think determines the best clusters.  \n",
    "\n",
    "\n",
    "The evaluation part could invlove:\n",
    "- Printing out the top 15 most common words for each of the topics and seeing if they make sense.\n",
    "- Using the perplexity and log-likelihoood scores.\n",
    "- Using the pyLDAvis tool to investigate the different clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,topic in enumerate(best_lda_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(dtm)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(npr))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, dtm, cv)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK: Add a new column to the original quora dataframe that labels each question into one of the topic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = best_lda_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr['Topic'] = topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
